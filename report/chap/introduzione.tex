This assignment aims to evaluate and compare the results of two numerical methods for unconstrained optimization applied to various problems.
The method we decided to implement are the Nealder Mead method and the Modified Newton method. In particular for each problem we are going to run $11$ experiments for both methods, the first one is run with a given initial point and the others with perturbations of it. These experiments will contribute to compute some statistics that will help us to compare the method, such as the minimum value found, the number of successful runs, the number of iterations needed by the method to converge and the experimental rate of convergence.

We can now proceed giving some details on how the algorithms have been implemented.

\subsection*{Nealder Mead Method}
The Nelder-Mead Method is a derivative-free optimization technique that minimizes the objective function by evaluating it at the vertices of a simplex, which it uses to navigate the multidimensional space.
The method employs four main operations: reflection, expansion, contraction, and shrinking, each of them is controlled by a parameter $\rho$ for reflection, $\chi$ for expansion, $\gamma$ for contraction, and $\sigma$ for shrinking. These parameters are provided as inputs to the algorithm to optimize performance for each specific problem.

The initial simplex is constructed by perturbing the initial point, as we are not given an initial simplex. Specifically, the $i^{th}$ vertex is generated by perturbing one component of the initial point, formulated as $x_{i} = x_0 + a e_i$. These perturbations are not fixed; for each vertex, we evaluate few values of $a$ in order to minimize the objective function computed in $x_i$, aiming to get closer to the minimum point and obtaining a faster convergence to it.

The stopping criterion for the method is based on the tolerance of the current simplex size. Ideally, the simplex shrinks as it approaches the minimum, so once the simplex has significantly reduced in size, we consider the method to have converged.

This algorithm is heuristic, meaning that convergence to a global minimum is not guaranteed, as it may get stuck in a non-stationary point.


\subsection*{Modified Newton Method}
The second method we have implemented is the Modified Newton Method, which leverages the gradient and the Hessian matrix of the function to move along descent directions at each iteration to reach the minimum.

This method faces two main challenges: performing an efficient line search at each iteration and ensuring the Hessian matrix is positive definite to compute the descent direction.

To address the first issue, we use a backtracking strategy for the line search. The parameters for the Armijo conditions and the maximum number of backtracking steps varies for each problem to optimize performance.

For the second issue, we regularize the Hessian matrix by adding a positive quantity to its diagonal until it becomes positive definite. We determine the positive definiteness by attempting a Cholesky factorization, which, if successful, is also used to solve the linear system to find the descent direction.

The stopping criterion is bases on the tolerance of the norm of the gradient.


\subsection*{Rate of Convergence}
As said before, a way to evaluate the performance of a numerical method is to compute the rate of convergence. In some cases, this could not be possible because in its definition appears the minimum of the found which is not known a priori.
Then what we will compute is the experimental rate of convergence which is defined as 
\begin{equation}
    q \approx \frac{ \log \left(\frac{\| \hat{e}^{(k+1)} \|}{\| \hat{e}^{(k)} \|} \right)}{ \log \left(\frac{\| \hat{e}^{(k)} \|}{\| \hat{e}^{(k-1)} \|} \right)}
    \quad \text{ for $k$ large enough}
    \label{definizione_roc}
\end{equation}
where $\hat{e}^{(k)} = x^{(k)} - x^{(k-1)}$ approximates the error at the $k$-th iteration (if the exact solution is unknown).
This approximation is valid for $k$ large enough, so it is possible that the value we compute may not be entirely reliable.
